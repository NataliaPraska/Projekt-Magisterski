\chapter{Projektowanie rozwiązań i Implementacja}

\section{Projektowanie rozwiązań}
\subsection{ Założenia koncepcyjne i wybór gatunku gry}

 Projektowany prototyp został zaplanowany jako środowisko testowe w formie gry VR typu puzzle/exploration. Jego głównym celem nie jest rozbudowana rozgrywka fabularna, lecz umożliwienie oceny i porównania rożnych podejść do projektowania interfejsu użytkownika oraz interakcji w VR. Wybór tego gatunku był spowodowany możliwością bezproblemowego osadzenia zadań interakcyjnych w przestrzeni, bez konieczności stosowania skomplikowanych mechanik, które nie należą do badanego zakresu. Postanowiono również o wyłączeniu obszaru związanego z mechaniką ekwipunku, ponieważ pomimo początkowych przypuszczeń na etapie heurystycznego badania eksperckiego wyniki badan nie wskazywały na to, że ten element stanowi dla użytkowników problem krytyczny. Decyzja o przeprowadzeniu eksperymentu wynikała z potrzeby konfrontacji wizji projektowej z rzeczywistym doświadczeniem użytkowników. Nawet niewielka grupa badanych może ujawnić krytyczne problemy interakcyjne, które pozostają niewidoczne na etapie projektowania i implementacji, co uzasadnia włączenie etapu empirycznego jako integralnej części procesu projektowego \cite{Badania-graczy}.

Założeniem koncepcyjnym realizowanego projektu było zaprojektowanie środowiska, w którym użytkownik wykonuje serię z góry ustalonych zadań, wymagających wykorzystania m.in. nawigacji, manipulacji obiektami oraz podejmowania prostych decyzji. Taka forma rozgrywki pozwoliła ograniczyć wpływ zewnętrznych czynników, takich jak presja czasu czy element rywalizacji. W projekcie zrezygnowano z możliwości poniesienia porażki, co miało umożliwić pełniejsze skoncentrowanie się na sposobie obsługi interfejsu i przebiegu interakcji, niezależnie od poziomu doświadczenia użytkownika. Takie rozwiązanie wymagało uwzględnienia w procesie projektowym zarówno przystępności prototypu, jak i budowania poczucia sprawczości, których brak u osób niedoświadczonych może prowadzić do odrzucenia technologii VR \cite{Ewaluacja-UX}.

\subsection{Charakterystyka porównywanych wariantów (Wariant A i B)}

Na podstawie analizy heurystycznej oraz badania ankietowego zdecydowano się na implementacje dwóch odmiennych wariantów interfejsu użytkownika. Różnią się one podejściem do projektowania UI w środowisku wirtualnym, a ich celem nie było stworzenie lepszego i gorszego rozwiązania, lecz porównanie podejścia konwencjonalnego z rozwiązaniem autorskim, zaprojektowanym w oparciu o pozyskane wytyczne projektowe.

Wariant A w dużej mierze bazuje na powszechnie wykorzystywanych rozwiązaniach znanych z gier VR. Cechuje go użycie ekranowych paneli interfejsu, które obracają się zgodnie z kierunkiem patrzenia użytkownika i są najczęściej obsługiwane przy pomocy wskaźnika laserowego. Lokomocja realizowana jest w sposób ciągły, z płynnym obrotem postaci. Interfejs funkcjonuje w nich głównie jako warstwa ekranowa, niezależna od obiektów świata gry. Informacje prezentowane są w postaci półprzezroczystych paneli wyświetlanych przed użytkownikiem, które w niewielkim stopniu mogą przysłaniać pole widzenia i pełnią rolę głównego nośnika informacji. Menu główne oraz ekrany pomocnicze mają formę dużych paneli ekranowych, których obsługa odbywa się za pomocą laserowego wskaźnika, a w trakcie interakcji z obiektami świata mogą pojawiać się proste okna kontekstowe.

Wariant B jest autorskim podejściem i został zaimplementowany jako implementacja wniosków uzyskanych z wcześniejszych rozdziałów badawczych. Interfejs jest zintegrowany z ciałem użytkownika oraz elementami wirtualnego środowiska, przyjmując formę fizycznie istniejących elementów, takich jak dziennik czy zegarek na nadgarstku. Dodatkowe informacje i wskazówki są prezentowane bezpośrednio w przestrzeni wirtualnej, m.in. na ścianach i elementach otoczenia. Mogą to być także fizyczne notatki umieszczone obok obiektów interaktywnych, które sugerują sposób użycia przedmiotów lub kierunek dalszego działania.

Interakcja opiera się na bezpośrednim dotyku i manipulacji obiektami. Każdej akcji towarzyszy zsynchronizowana z nią wielozmysłowa informacja zwrotna, która istotnie wzmacnia realizm interakcji i poczucie obecności w VR. Wynika to z analizy heurystycznej oraz wniosków przedstawionych w literaturze \cite{Prototypy-XR,Projektowanie}.

\subsection{Projekt scenariuszy zadań testowych}

W ramach projektowanego środowiska testowego przygotowano 4 zadania interakcyjne, które będą realizowane przez użytkownika w określonej z góry kolejności. Ich celem jest obserwacja sposobu korzystania z interfejsu oraz mechanizmów interakcji w wirtualnym środowisku. Każdy ze scenariuszy został zaprojektowany w taki sposób, aby w danym zadaniu badany był tylko jeden obszar, a pozostałe elementy pozostawały niezmienne, pełniły role tła i zapewniały kontekst użycia. Oznacza to, że różnice miedzy wariantami A i B dotyczą tylko jednego aspektu, np. w przypadku zadania 1 jest to interfejs użytkownika, mechanizmy lokomocji i feedback pozostają bez zmian. takie podejście miało ograniczyć wpływ innych obszarów na aktualnie badany i zapewnić bardziej jednolite porównanie.

\subsubsection{Zadanie 1: Sortowanie obiektów}

Pierwsze zdanie miało bezpośrednio ocenić interfejs użytkownika i jego wpływ na czas realizacji zadania, radzenie sobie z odnajdywaniem potrzebnych informacji oraz wrażenie obecności. Zadanie rozpoczyna się od momentu wyjścia z windy. Po przejściu kilku kroków przed użytkownikiem, w zależności od wariantu, pojawia się interfejs panelowy lub interfejs umieszczony w fizycznym obiekcie. Otrzymuje informacje o celu zadania, po czym rozpoczyna eksploracje pomieszczenia, w którym się znajduje, w poszukiwaniu trzech obiektów rozmieszczonych w różnych miejscach w pokoju. Pierwszy z obiektów jest łatwo widoczny, drugi ukryty w szafie, zaś trzeci jest umieszczony na podwyższeniu i wymaga interakcji z dźwignia, aby opuścić półkę niżej. Zadanie pozwala zaobserwować, w jaki sposób użytkownik korzysta z interfejsu, czy potrzebuje powrotu do dostępnych elementów informacyjnych oraz jak szybko jest w stanie zrozumieć i wykonać zadanie. W obu wariantach obiekty są chwytane bezpośrednio przez wirtualne ręce użytkownika i przenoszone na odpowiednią platformę. Główne różnice między oboma wariantami wynikają ze sposobu prezentowania informacji o postępie w wykonywanym działaniu, a samo zadanie umożliwia obserwację, jak interfejs prowadzi użytkownika do celu, wspiera eksplorację i podstawową interakcje z obiektami.

\subsubsection{Zadanie 2: Zapamiętanie i odtworzenie sekwencji}

Głównym celem drugiego zadania była ocena intensywności informacji zwrotnej, a nie formy jej prezentacji. Przy projektowaniu uwzględniono naturalne ograniczenia pamięci użytkowników. W obu wersjach interfejs został umieszczony w środowisku wirtualnym, z zachowaniem identycznego sposobu poruszania się, aby w jak największym stopniu skupić uwagę użytkownika na analizowanym obszarze. Zadanie również nie uległo żadnym zmianom, a jedyną różnicą miedzy nimi była ilość wykorzystywanych kanałów feedbacku oraz ich intensywność. W wariancie A informacja zwrotna ograniczona jest do pojedynczego, nisko-intensywnego kanału dźwiękowego, natomiast w wariancie B zastosowano informację zwrotną wielokanałową, łączącą bodźce wizualne, dźwiękowe oraz haptyczne. Pozwalało to na ocenę wpływu niskiej i wysokiej intensywności feedbacku na przebieg i prędkość wykonania zadania.

\subsubsection{Zadanie 3: Lokomocja po platformach}

Zadanie trzecie skupiało się na mechanizmach lokomocji. Użytkownik przemieszcza się po torze przeszkód złożonych z niewielkich platform. W wariancie A zastosowano lokomocje ciągłą, natomiast w wariancie B teleportacje punktową. W zadaniu tym analizowany jest nie tylko czas potrzebny na ukończenie planszy, ale również liczba upadków. Zastosowane metryki miały pozwolić na ocenę stabilności i bezpieczeństwa wybranego sposobu przemieszczania się. Daje także możliwość ewaluacji wpływu na tempo poruszania się użytkownika.

\subsubsection{Zadanie 4: Aktywacja światła i wspinaczka}

Czwarte zadanie integruje wszystkie wcześniej analizowane aspekty. Obejmuje pracę z interfejsem, podstawowe i bardziej zaawansowane sposoby lokomocji, łączące wybrany rodzaj ruchu z fizycznym przemieszczaniem się użytkownika. Uwzględnia także manipulowanie obiektami oraz odbiór informacji zwrotnej. W wariancie A użytkownik porusza się po środowisku za pomocą płynnego ruchu, natomiast w wariancie B korzysta z teleportacji. W obu trybach możliwy jest również fizyczny ruch. Analizie poddano czas potrzebny na ukończenie całej planszy, wszystkie upadki oraz problemy z odnalezieniem właściwej trasy, a samo zadanie miało charakter podsumowujący i pozwalało zaobserwować ogólny sposób funkcjonowania użytkownika w bardziej złożonym i wymagającym środowisku.

\section{Implementacja}
\subsection{Środowisko i konfiguracja projektu Unity VR}

Do celów implementacyjnych wykorzystano silnik Unity 3D, który wyróżnia się szczegółową dokumentacją i szeroką bazą paczek umożliwiających integrację z rzeczywistością wirtualną. Twórcy silnika przygotowali również specjalny szablon projektu, zawierający integrację z paczką Oculus Meta oraz przykładową scenę z interaktywnymi obiektami i instrukcją.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{images/chapter5/5_1.png}
    \caption{Przykładowa scena w Unity z szablonu wirtualnej rzeczywistości}
    \label{unity_vr_template}
\end{figure}

Bardzo specyficzne jest tworzenie aplikacji w wirtualnej rzeczywistości, ponieważ rozgrywka opiera się głównie na wykorzystaniu specjalnych okularów VR oraz joysticków, które są wirtualnymi rękami. Ich brak uniemożliwia poprawne testowanie interakcji ze światem. Ponadto regularne weryfikowanie zmian wymaga od użytkownika częstego przenoszenia się z ekranu monitora na tryb VR, gdzie każde zakładanie gogli może być męczące.

W tym celu stworzono tryb symulacyjny, który pozwala na testowanie w uproszczony sposób wszystkich elementów wirtualnego awatara bezpośrednio przy użyciu klawiatury i myszy. Nie jest to zbyt intuicyjne rozwiązanie z uwagi na to, ile funkcji musi zostać tutaj obsłużonych, ale pozwala skupić się na tworzeniu funkcjonalności i wstępnym testowaniu rozgrywki, zanim aktualizacja trafi do działu testerów.

Przykładowa scena testowa widoczna na obrazku powyżej (\ref{unity_vr_template}) zawiera film, prosty dwuwymiarowy interfejs użytkownika oraz kilka interaktywnych elementów, takich jak strzelający pistolet, skalowalna piłka oraz obracalna kula umieszczona sztywno na piedestale. Pozwala to zapoznać się z podstawową interakcją oraz zobaczyć, jakie komponenty są użyte, aby móc stworzyć samodzielnie podobny obiekt.

\subsection{Architektura środowiska VR i struktura scen}

Aby utworzyć odpowiednie środowisko testowe, należy wybrać odpowiedni typ gry, który w łatwy i naturalny sposób umożliwi wdrożenie odpowiednich scenariuszy do badań. Dlatego nie sprawdzi się tutaj gatunek akcji lub gra rytmiczna, gdzie przede wszystkim ważna jest dynamika i efekty audiowizualne. Najbardziej odpowiednią platformą do przeprowadzenia badań jest coś z wolniejszą narracją i niskim progiem wejścia.

Wybrany został gatunek o nazwie \textit{exploration puzzle game}, który w odpowiednio uproszczonym prototypie umożliwi przetestowanie różnych podejść do projektowania interfejsu użytkownika oraz przeprowadzenie badań na przygotowanych scenariuszach.

Rozgrywka została podzielona na etapy, które oferują inny typ środowiska oraz niezależne wyzwania, które umożliwią zweryfikowanie, ile czasu zajmie wykonanie użytkownikom danego zadania oraz ile prób będą wymagać, aby ukończyć cały etap. Każdy etap oferuje dwa tryby integracji interfejsu w świecie gry, aby dokładniej zbadać zalety i wady każdego rozwiązania. Trzeci tryb to połączone rozwiązania, mające na celu znalezienie złotego środka i udowodnienie, że najlepszym rozwiązaniem jest umiejętne wykorzystanie obu technik.

Pierwszy sposób integracji interfejsu użytkownika opiera się na latających dwuwymiarowych diagramach, które zawsze są obrócone w kierunku gracza i umożliwiają sterowanie poszczególnymi elementami interfejsu poprzez specjalne lasery wydobywające się z palców wskazujących gracza. Ten sam sposób został wykorzystany na scenie testowej na zdjęciu \ref{unity_vr_template} umieszczonym powyżej.

Drugi sposób opiera się na maksymalnym wykorzystaniu fizycznych obiektów w świecie gry, aby zwiększyć imersję użytkownika i wykorzystać w pełni potencjał wirtualnej rzeczywistości. Zamiast płaskiego interfejsu część opcji ukryta jest w interaktywnej książce wyciąganej z pasa awatara, a zamiast latających diagramów poszczególne wskazówki zostają zintegrowane pod postacią listów i napisów na ścianach.

Technika ta jest również częściowo wykorzystywana w zwykłych grach, jednak największą różnicą jest to, że tam użytkownik nie ma możliwości naturalnej interakcji z takimi obiektami. W wirtualnej rzeczywistości Znacznie ciekawsze jest podniesienie listu i przeczytanie go fizycznie trzymając ten obiekt w rękach zamiast szukać odpowiedniego przedmiotu w swoim ekwipunku będącego reprezentacją dwuwymiarową na ekranie.

\subsection{Model interakcji użytkownika w środowisku VR}

W odróżnieniu od tradycyjnych gier, interakcja w VR opiera się na przestrzennym i interaktywnym działaniu użytkownika wykorzystującym ruchy głowy, rąk, a nawet całego ciała, aby na przykład schylić się po jakiś przedmiot. Pozwala to na bardzo realistyczne podejście do tematu i obsłużenie wszystkich przypadków, co przydaje się przede wszystkim w symulatorach medycznych, gdzie bardzo ważne jest odwzorowanie każdego zachowania.

Przyjęty model interakcji opiera się na cyklu, gdzie proces interakcji rozpoczyna się od intencji użytkownika. Dana osoba musi dokładnie wiedzieć, co może zrobić oraz znać swoje ograniczenia, aby nie próbować podejmować czynności, które nie zostały przewidziane przez twórców aplikacji.

Następnie następuje realizacja intencji poprzez fizyczną akcję, która przyjmuje formę konkretnego gestu lub działania, np. pociągnięcie dźwigni lub pobiegnięcie do celu. W tym celu użytkownik musi odpowiednio się obrócić, odpowiednio ułożyć pozę ciała i kliknąć odpowiednie przyciski na joysticku, aby zarejestrować odpowiednie akcje.

Ostatnim etapem cyklu jest sama reakcja systemu, która stanowi informację zwrotną dla użytkownika. Może się to objawiać poprzez zmianę stanu świata gry oraz komunikaty wizualne i dźwiękowe. Istotne jest, aby użytkownik na ich podstawie wiedział, że wykonał czynność poprawnie oraz czego może się po tej akcji spodziewać. W zwykłych grach może się to objawiać poprzez odtworzenie krótkiej animacji, gdzie kamera przelatuje przez planszę i pokazuje otwarte drzwi. W wirtualnej rzeczywistości nie jest to możliwe.

Model interakcji został zaprojektowany w oparciu o jasno określone ograniczenia i możliwości awatara. Użytkownik może wchodzić w interakcję wyłącznie z elementami jednoznacznie oznaczonymi jako interaktywne oraz wykonywać akcje zgodne z kontekstem aktualnego etapu, na przykład wspinać się dopiero na ostatnim etapie. Użytkownik może do tego wykorzystać różne formy interakcji, takich jak wskazywanie, chwytanie oraz aktywacja obiektów.

Nie jest jednak możliwe, aby użytkownik mógł wchodzić w interakcję z elementami pełniącymi wyłącznie funkcję dekoracyjną oraz wpływać na logikę zadań w sposób omijający zaprojektowane mechaniki gry. Dlatego elementy interaktywne powinny na pierwszy rzut oka być bardziej istotne i ciekawe dla użytkownika

Istotną formą interakcji jest lokomocja, czyli poruszanie się wykorzystując ruch ciągły oraz teleport. Umożliwia to sterowanie w sztuczny sposób wirtualnym awatarem, gdyż sam użytkownik ma ograniczony obszar ruchu.

\subsection{Implementacja interaktywnych elementów i technik interakcji}

Zintegrowana z szablonem projektu paczka integracyjna do wirtualnej rzeczywistości zawiera podstawowe komponenty, które umożliwiają nadanie odpowiednich funkcjonalności surowym obiektom na scenie. Całość jest zintegrowana z ekosystemem wirtualnej rzeczywistości oraz wspiera dodatkowe funkcjonalności, takie jak niestandardowe gesty joysticków. Przy implementacji nowych systemów istotne jest opieranie się na gotowych rozwiązaniach, które można w dowolny sposób rozszerzać i modyfikować.

Umożliwia to szybsze wdrożenie się w wirtualną rzeczywistość i skupienie się na implementacji rozgrywki zamiast tracić czas na przygotowywanie dokładnej integracji z modułem VR. Sprawia to, że próg wejścia w tę technologię jest znacznie niższy.

Istnieją również alternatywne paczki do obsługi wirtualnej rzeczywistości, które oferują szerszy pakiet użytecznych funkcji i zasobów. Użyta w projekcie paczka \textit{AutoHand} wspiera automatyczne układanie wirtualnej dłoni na obiektach na podstawie kolizji z interaktywnym obiektem bez ręcznego definiowania pozycji palców, co znacząco przyśpiesza pracę uzyskując bardzo dobry efekt. Dodatkowo autor paczki przygotował rozbudowane środowisko testowe z dużą bazą interaktywnych obiektów, które samo w sobie jest już bardzo dobrym scenariuszem do nauki wirtualnej rzeczywistości i jego pełnych możliwości (rysunek \ref{autohand_example}).

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter5/AutoHand.PNG}
    \caption{Scena symulacyjna od twórcy paczki AutoHand}
    \label{autohand_example}
\end{figure}

\subsection{Architektura interfejsu użytkownika}

W środowiskach VR interfejs użytkownika nie jest wyłącznie warstwą graficzną, lecz częścią przestrzennej interakcji. W odróżnieniu od klasycznych aplikacji ekranowych, elementy UI mogą być umieszczone bezpośrednie w świecie, tradycyjnie przypięte do użytkownika, zmaterializowane jako obiekty diegetyczne lub realizowane jako klasyczne panele 2D renderowane w przestrzeni. Literatura dotycząca projektowania interakcji immersyjnych podkreśla, że dobór technik UI powinien wynikać z rodzaju zadania, kosztów poznawczych oraz ograniczeń percepcyjnych (np. czytelności, komfortu i obciążenia uwagi) \cite{Projektowanie-VR}.

Z perspektywy architektury systemu kluczowe jest rozdzielenie \textit{treści i logiki} od \textit{warstwy prezentacji}. Ten sam komunikat lub wskazówka dla gracza (np. „podpowiedź do rozwiązania łamigłówki”) może zostać przedstawiony w dwóch równoległych implementacjach: jako element diegetyczny w świecie (np. kartka z tekstem lub ekran laptopa) oraz jako klasyczny panel 2D (np. hologram/okno w przestrzeni). Taki podział umożliwia porównanie wpływu sposobu prezentacji na użyteczność, przy zachowaniu tej samej zawartości i celu zadaniowego.

\subsubsection{Interfejs dwuwymiarowy (\textit{world-space UI})}

Interfejs 2D w wirtualnej rzeczywistości realizowany jest najczęściej jako panel umieszczony w przestrzeni trójwymiarowej sceny (\textit{world-space canvas}), a nie bezpośrednio na soczewkach gogli. W silniku Unity odpowiada temu komponent \textit{Canvas} z trybem renderowania \textit{World Space}, który pozwala osadzić płaską powierzchnię z kontrolkami w dowolnym miejscu sceny.

Interakcja z elementami takiego panelu odbywa się zazwyczaj za pomocą promienia wskazującego (\textit{ray pointer}) emitowanego z kontrolera ruchowego, co stanowi analogię do wskazywania laserem na tablicy interaktywnej. Użytkownik kieruje promień na wybrany element interfejsu i wchodzi w interakcję poprzez przyciski, co zapewnia intuicyjny i precyzyjny sposób obsługi kontrolek znany z konwencjonalnych interfejsów graficznych \cite{LaViola2000}.

Interfejs 2D opiera się na wykorzystaniu komponentu \texttt{canvas} w trybie \textit{World Space}. W odróżnieniu od trybu \textit{Screen Space}, w którym elementy UI są renderowane jako nakładka na obraz kamery, tryb \textit{World Space} pozwala umieścić płaszczyznę interfejsu jako obiekt w trójwymiarowej scenie, nadając jej pozycję, rotację i skalę niezależne od kamery (rysunek \ref{ui_2d_example}). Dzięki temu panel z kontrolkami może być osadzony w konkretnym miejscu świata gry lub dynamicznie podążać za użytkownikiem.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter5/ui_2d_example.png}
    \caption{Przykład interfejsu 2D. Latający canvas w przestrzeni world-space}
    \label{ui_2d_example}
\end{figure}

Aby zapewnić czytelność i wygodę użytkowania, panel może być skonfigurowany tak, aby podążał za orientacją głowy użytkownika z lekkim opóźnieniem (skrypt \textit{lazy follow} na listingu \ref{lazy_follow}). Mechanizm ten, stosowany między innymi w grach \textit{Beat Saber} czy \textit{Boneworks}, zapobiega sytuacji, w której użytkownik musiałby się odpowiednio ustawić, aby móc zobaczyć wszystkie informacje, a jednocześnie unika nieimersyjnego uczucia sztywnego przytwierdzenia do pola widzenia, co mogłoby powodować efekt choroby lokomocyjnej \cite{LaViola2000}.

\begin{center}
\lstinputlisting[basicstyle=\ttfamily\footnotesize,label={lazy_follow}, caption=Skrypt opóźnionego podążania panelu UI za kierunkiem wzroku gracza., firstline=5, lastline=26]{listings/LazyFollow.cs}
\end{center}

\subsubsection{Interfejs trójwymiarowy (diegetyczny)}

Drugie podejście polega na osadzeniu informacji bezpośrednio w obiektach świata gry, co zwiększa immersję i spójność narracyjną. Przykładem takiego interfejsu jest kartka papieru zawierająca wskazówki dla gracza lub wplecioną fabułę rozgrywki, interaktywna książka, której strony można przewracać, czy laptop umieszczony w scenie, na ekranie którego wyświetlane są dane (\cite{Fagerholt2009}). Podejście to jest charakterystyczne dla produkcji stawiających na realizm interakcji, takich jak \textit{Half-Life: Alyx} firmy \textit{Valve} lub \textit{Profundum} stworzonej przez \textit{Black Mouse}, gdzie większość informacji przekazywana jest poprzez interaktywne obiekty fizycznie obecne w otoczeniu gracza.

Interfejs diegetyczny wymaga od projektanta większego nakładu pracy, ponieważ każdy element informacyjny musi zostać zintegrowany z geometrią i logiką sceny. Wiąże się to z koniecznością opracowania wiarygodnych modeli 3D, animacji oraz dedykowanych mechanizmów interakcji (na przykład chwytanie, obracanie, przybliżanie obiektów). W zamian użytkownik otrzymuje doświadczenie, w którym granica między interfejsem a światem gry ulega zatarciu, co jest jednym z głównych celów projektowania w VR (\cite{Projektowanie-VR,Interfejsy-3D}).

Najprostszą formą interfejsu diegetycznego jest umieszczenie na scenie modelu urządzenia wyposażonego w ekran, na przykład komputera. Ekran takiego obiektu stanowi idealną bazę pod komponent \textit{Canvas} w trybie \textit{World Space} dopasowany do ekranu urządzenia. Renderowanie elementów UI odbywa się identycznie jak w przypadku interfejsu 2D, jednak kontekst przestrzenny (fizyczna obecność urządzenia, konieczność podejścia do niego) nadaje interakcji charakter diegetyczny. Interakcja może odbywać się zarówno za pomocą promienia wskazującego z~dystansu, jak i poprzez bezpośrednie wskazywanie wirtualnym palcem.

Najbardziej złożoną formą interfejsu diegetycznego jest obiekt wymagający fizycznej manipulacji. Dobrym przykładem jest książka, której kartki użytkownik może chwytać i przewracać, co zostało przedstawione w grze \textit{Profundum} na rysunku \ref{ui_3d_example}. Implementacja takiego elementu obejmuje kilka warstw:
\begin{itemize}
    \item \textbf{Geometria i animacja} — każda strona stanowi osobny mesh z dedykowanym szkieletem animacji (\textit{rig}) lub jest kontrolowana proceduralnie (na przykład poprzez \textit{blend shapes} odwzorowujące zginanie kartki). Przejście między stronami może być wyzwalane gestem chwytania i przeciągania.
    \item \textbf{Treść stron} — zawartość poszczególnych stron może być realizowana jako \textit{Render Texture}, jeśli wymagana jest dynamiczna aktualizacja lub jako zestaw przygotowanych tekstur przypisanych do materiałów kolejnych stron, gdy treść jest statyczna.
    \item \textbf{Logika nawigacji} — skrypt zarządzający stanem książki śledzi aktualnie otwartą stronę, obsługuje animację przewracania oraz aktualizuje stan w zależności od bieżącej strony.
\end{itemize}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter5/ui_3d_example.png}
    \caption{Przykład interfejsu diegetycznego w formie interaktywnej książki w grze \textit{Profundum}}
    \label{ui_3d_example}
\end{figure}

Tego rodzaju interfejs wymaga znacznie większego nakładu implementacyjnego niż zwykły panel 2D, jednak oferuje wysoki poziom immersji i jest spójny z paradygmatem naturalnej interakcji, w którym użytkownik manipuluje obiektami w sposób analogiczny do świata rzeczywistego \cite{Interfejsy-3D}.

\subsubsection{Podejście hybrydowe}

W praktyce projektowej najkorzystniejsze rezultaty przynosi połączenie obu paradygmatów. Interfejs 2D sprawdza się tam, gdzie kluczowa jest szybkość odczytu informacji i precyzja obsługi kontrolek, na przykład w menu ustawień, listach wyboru czy panelach statystyk. Interfejs 3D jest natomiast preferowany w sytuacjach, w których immersja i zaangażowanie użytkownika mają priorytet, a nadmiar płaskich elementów mógłby zaburzać poczucie obecności w wirtualnym świecie \cite{Interfejsy-3D}.

Badania nad użytecznością interfejsów w VR potwierdzają, że podejście hybrydowe, dobierające formę prezentacji do kontekstu i złożoności przekazywanej informacji, pozwala uzyskać lepszą równowagę między czytelnością a immersją niż zastosowanie wyłącznie jednego z paradygmatów \cite{Dachselt2017,Projektowanie-VR}. Przykład wykresów opartych na obu podejściach przedstawiony został na rysunku \ref{ui_mix_example}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{images/chapter5/ui_mix_example.png}
    \caption{Przykład wykresów 2D i 3D}
    \label{ui_mix_example}
\end{figure}

W ramach niniejszego projektu badawczego opracowano dwa scenariusze testowe: pierwszy wykorzystuje wyłącznie interfejs 2D, natomiast drugi opiera się głównie na elementach diegetycznego interfejsu 3D. Oba scenariusze realizowane są na identycznie zaprojektowanym poziomie i obejmują ten sam zestaw zadań, co pozwala na bezpośrednie porównanie wpływu formy interfejsu na doświadczenia uczestników badania w kontrolowanych warunkach.

\subsection{System zadań badawczych i przechodzenie miedzy etapami}

Rzetelne zbadanie każdego scenariusza testowego oraz wyciągnięcie trafnych wniosków wymaga szczegółowego rejestrowania danych z przeprowadzonych symulacji. System pomiarowy musi odnotowywać dokładny moment rozpoczęcia każdej sekwencji, mierzyć czas jej trwania oraz weryfikować warunek końcowy pozwalający uznać daną sekwencję za ukończoną.

W tym celu opracowano system zadań zintegrowany z obiektami przeszkód. Dopóki przypisane zadanie nie zostanie wykonane, blokada pozostaje zamknięta, uniemożliwiając użytkownikowi dostęp do dalszej części poziomu. Mechanizm ten wymusza sekwencyjne przechodzenie przez kolejne etapy i umożliwia precyzyjne obliczenie czasu potrzebnego na realizację każdego zadania.

\subsubsection{Implementacja systemu sekwencyjnych zadań}

Klasa TaskSequence przedstawiona na listingu \ref{TaskSequence} opiera się na obiektach \textit{Checkpoint}. Zadanie nasłuchuje globalnych zdarzeń i oczekuje, aż wszystkie przypisane do niej punkty kontrolne zostaną zrealizowane, po czym zmienia swój stan na ukończony. Komponent \textit{Checkpoint} może zostać dołączony do dowolnego obiektu interaktywnego lub do wyznaczonej przestrzeni wyzwalającej się w momencie wejścia gracza w jej obszar. Prostota tej implementacji pozwala powiązać punkt kontrolny z różnorodnymi interakcjami VR, na przykład z odłożeniem przedmiotu we wskazane miejsce, trafieniem w cel czy wprowadzeniem odpowiedniej kombinacji przycisków.

\begin{center}
\lstinputlisting[basicstyle=\ttfamily\footnotesize,label={TaskSequence}, caption=Klasa \textit{TaskSequence} generująca statystyki na temat wykonanego zadania., firstline=21, lastline=48]{listings/TaskSequence.cs}
\end{center}

Wszystkie zadania są zarządzane przez globalny komponent \textit{SequenceManager}, który po załadowaniu scenariusza wyszukuje na scenie wszystkie obiekty klasy \textit{TaskSequence}, aktywuje domyślnie pierwszy w ustalonej kolejności, a następnie gromadzi informacje o wyniku każdego zadania. Po ukończeniu sekwencji utworzony obiekt klasy \textit{SequenceResult} jest przekazywany do komponentu \textit{Elevator}, który kieruje dane podsumowujące do graficznego interfejsu w celu wyświetlenia ich na ekranie podsumowania po zakończeniu poziomu.

\begin{center}
\lstinputlisting[basicstyle=\ttfamily\footnotesize,label={SequenceManager}, caption=Logika klasy \textit{SequenceManager} odpowiadająca za zarządzanie zadaniami., firstline=12, lastline=42]{listings/SequenceManager.cs}
\end{center}

Aby zapewnić wizualne odwzorowanie zrealizowanych etapów danego zadania, opracowano system dynamicznie tworzonych obiektów klasy \textit{CheckpointIndicator}. Zmieniają one kolor z czerwonego na zielony po ukończeniu przypisanych do nich Checkpointów. Obiekty te generowane są zarówno na docelowej blokadzie, jak i na dłoni awatara w formie wbudowanego panelu podręcznego. Kluczowym aspektem było zagwarantowanie, by w zależności od stopnia złożoności zadania pojawiała się właściwa liczba trójwymiarowych kontrolek we właściwych miejscach, co wymagało zaimplementowania logiki opartej na mechanizmie "\textit{HorizontalLayoutGroup}", znanym z projektowania interfejsów 2D w silniku Unity (listing \ref{CheckpointIndicatorSpawn}).

\begin{center}
\lstinputlisting[basicstyle=\ttfamily\footnotesize,label={CheckpointIndicatorSpawn}, caption=Metody generujące graficzne reprezentacje checkpointów na blokadzie i ręce gracza, firstline=49, lastline=81]{listings/TaskSequence.cs}
\end{center}

\subsubsection{Mechanizm windy jako element przejściowy}

W zaprojektowanym środowisku VR przechodzenie pomiędzy kolejnymi etapami gry oparto na jednolitym systemie wykorzystującym motyw windy. Mechanizm ten spełnia kilka ról jednocześnie: stanowi element świata przedstawionego, umożliwia przejście między etapami, odpowiada za mechanizm ładowania kolejnych scen oraz pełni funkcję ekranu podsumowującego wykonane zadania. Takie podejście pozwoliło połączyć wymagania techniczne z założeniami projektowymi dotyczącymi immersji oraz przejrzystości interfejsu, co opisano w książce *Dizajn na co dzień* \cite{Odczucie-gry}.

Zastosowanie windy jako elementu przejściowego było podyktowane kilkoma względami projektowymi. Przede wszystkim to rozwiązanie ogranicza dezorientację użytkownika, która w środowiskach VR często pojawia się podczas nagłych zmian scen lub wyświetlania pełnoekranowych interfejsów. Ponadto zapewnia ciągłość doświadczenia, ponieważ użytkownik przez cały czas pozostaje w obrębie świata gry. Winda tworzy również naturalny moment zatrzymania akcji, sprzyjający refleksji nad wykonanymi zadaniami. W porównaniu ze standardowymi ekranami podsumowania stosowanymi w grach, to rozwiązanie jest lepiej dostosowane do specyfiki VR, w której oderwanie użytkownika od trójwymiarowej przestrzeni świata gry może negatywnie wpływać na immersję i komfort użytkowania, a w skrajnych przypadkach przyczyniać się do wystąpienia objawów choroby symulatorowej.

\subsubsection{Implementacja systemu windy}

System windy oparty jest na globalnym menedżerze przechowywanym w specjalnym stanie gry \textit{DontDestroyOnLoad}, dzięki czemu obiekt zostaje zachowany podczas przechodzenia między scenami. Gotowy prefabrykat zawiera również model wnętrza windy bez drzwi. Po załadowaniu nowego poziomu instancja klasy \textit{Elevator} wyszukuje na scenie obiekt klasy \textit{ElevatorDock} o typie \textit{Start} i podłącza się do niego. \textit{ElevatorDock} jest komponentem osadzonym w osobnym prefabrykacie, stanowiącym front windy z animowanymi drzwiami, i to on zarządza logiką ich otwierania oraz zamykania. \textit{ElevatorManager} jedynie pozycjonuje się względem odpowiedniego doku.

Analogiczny mechanizm działa przy zakończeniu poziomu. W momencie, gdy użytkownik aktywuje przycisk otwarcia drzwi, obiekt \textit{Elevator} dołącza się do konkretnego \textit{ElevatorDock}, aby po otwarciu drzwi użytkownik zobaczył przed sobą wnętrze windy. Taki podział odpowiedzialności pozwala oddzielić windę od logiki konkretnego poziomu i nadać jej rolę globalnego zarządcy.

\begin{center}
\lstinputlisting[basicstyle=\ttfamily\footnotesize,label={ElevatorManager}, caption=Metoda w klasie \textit{ElevatorManager} ładująca asynchronicznie nowy poziom na korutynie., firstline=39, lastline=64]{listings/ElevatorManager.cs}
\end{center}

Ekran podsumowania generuje elementy interfejsu dynamicznie na podstawie liczby ukończonych zadań, wypełniając je danymi z obiektów typu \textit{SequenceResult}. Każdy taki obiekt zawiera nazwę zadania, czas realizacji poszczególnych etapów oraz liczbę popełnionych błędów. Zgromadzone w ten sposób dane służą do zebrania szczegółowych statystyk od grupy badawczej, umożliwiając identyfikację elementów, które sprawiają trudności niedoświadczonym użytkownikom i wymagają dalszych poprawek projektowych.